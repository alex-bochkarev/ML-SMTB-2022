{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389c3bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "086707e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input images to tensors and normalize\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Get the MNIST data from torchvision\n",
    "dataset1 = datasets.MNIST('./', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2 = datasets.MNIST('./', train=False,\n",
    "                    transform=transform)\n",
    "\n",
    "# Define the data loaders that will handle fetching of data\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size = 64)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size = 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a23a3557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439401a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "018ca99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct label: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = np.random.randint(1, 1000)\n",
    "\n",
    "img = dataset1[m][0]\n",
    "lbl = dataset1[m][1]\n",
    "\n",
    "plt.imshow(img[0])\n",
    "print(f\"Correct label: {lbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31460300",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(256, 64),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(64, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b2215f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "batch_size = 128\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc3e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] Loss: 2164.552\n",
      "Epoch [10] Loss: 2071.297\n",
      "Epoch [20] Loss: 1732.982\n",
      "Epoch [30] Loss: 1264.742\n",
      "Epoch [40] Loss: 950.619\n",
      "Epoch [50] Loss: 753.464\n",
      "Epoch [60] Loss: 623.621\n",
      "Epoch [70] Loss: 535.791\n",
      "Epoch [80] Loss: 475.198\n",
      "Epoch [90] Loss: 431.985\n",
      "Epoch [100] Loss: 400.104\n",
      "Epoch [110] Loss: 375.784\n",
      "Epoch [120] Loss: 356.604\n",
      "Epoch [130] Loss: 340.989\n",
      "Epoch [140] Loss: 327.903\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (batch_data, target) in enumerate(train_loader):\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "        pred_labels = model(batch_data)  # forward pass\n",
    "        \n",
    "        loss = criterion(pred_labels, target)\n",
    "\t\t\n",
    "        loss.backward()\n",
    "\n",
    "\t\t# Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "\t\t# Print progress\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch [{epoch}] Loss: {running_loss:.3f}\")\n",
    "\n",
    "print (\"\\n ### Finished Training ### \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ebccfe",
   "metadata": {},
   "source": [
    "## Let's test what we have got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.random.randint(1, 1000)\n",
    "\n",
    "img = dataset2[m][0]\n",
    "lbl = dataset2[m][1]\n",
    "\n",
    "plt.imshow(img[0])\n",
    "with torch.no_grad():\n",
    "    out = model.forward(img.view(img.size(0), -1)).float()\n",
    "\n",
    "print(f\"Correct label: {lbl}\")\n",
    "print(f\"Prediction: {out}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
